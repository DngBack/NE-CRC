# TÃ³m táº¯t tÃ­ch há»£p AbstentionBench tháº­t vÃ  sá»­a pipeline

## âœ… ÄÃ£ hoÃ n thÃ nh

### 1. TÃ­ch há»£p dataset tháº­t tá»« HuggingFace âœ“

**File**: `src/data/abstention_bench.py`

**Thay Ä‘á»•i**:
- ThÃªm method `_load_real_dataset()` Ä‘á»ƒ load tá»« `facebook/AbstentionBench`
- Tá»± Ä‘á»™ng fallback vá» synthetic data náº¿u load tháº¥t báº¡i
- Xá»­ lÃ½ nhiá»u format khÃ¡c nhau cá»§a dataset (field names, data types)
- Filtering theo scenario vá»›i logic linh hoáº¡t

**CÃ¡ch sá»­ dá»¥ng**:
```python
loader = create_default_loader()
samples = loader.load_dataset("unknown", use_real_data=True)  # Máº·c Ä‘á»‹nh True
```

### 2. Correctness Evaluator âœ“

**File**: `src/data/correctness_evaluator.py` (má»›i)

**TÃ­nh nÄƒng**:
- Evaluate correctness tá»« LLM outputs vs reference answers
- Support exact match, fuzzy match, semantic similarity
- Xá»­ lÃ½ cÃ¡c trÆ°á»ng há»£p:
  - Should abstain vÃ  model abstained â†’ correct (1.0)
  - Should abstain nhÆ°ng model answered â†’ incorrect (0.0)
  - Should answer nhÆ°ng model abstained â†’ incorrect (0.0)
  - Compare vá»›i reference answers

### 3. Cáº­p nháº­t pipeline vá»›i correctness evaluation âœ“

**File**: `src/pipeline/experiment.py`

**Thay Ä‘á»•i**:
- ThÃªm step 3: Evaluate correctness sau LLM inference
- Extract best answer tá»« generations (majority vote)
- Evaluate correctness dá»±a trÃªn reference answers vÃ  should_abstain
- Update Sample objects vá»›i correctness labels
- Pipeline giá» cÃ³ 9 steps thay vÃ¬ 8

**Flow má»›i**:
1. Load data
2. LLM inference
3. **Evaluate correctness** â† Má»šI
4. Extract features
5. Compute uncertainties
6. Prepare weights
7. Run systems
8. Evaluate metrics
9. Save results

### 4. Cáº£i thiá»‡n validation vÃ  logging âœ“

**ThÃªm vÃ o pipeline**:
- âœ… Validate data sau khi load (empty splits, minimum sizes)
- âœ… Log distribution cá»§a correctness labels
- âœ… Log calibration head performance (accuracy, confidence range)
- âœ… Log CRC threshold values
- âœ… Log SConU filter statistics (outlier rate)
- âœ… Log NE-CRC adaptive threshold statistics
- âœ… Log chi tiáº¿t metrics vá»›i risk violation warnings

**VÃ­ dá»¥ logs má»›i**:
```
Filtered samples with valid correctness labels:
  Train: 80/100 (80.0%)
  Calibration: 60/75 (80.0%)
  Test: 40/50 (80.0%)

Label distributions:
  Train label distribution: 45 correct, 35 incorrect (56.2% correct)
  Calibration label distribution: 30 correct, 30 incorrect (50.0% correct)
  Test label distribution: 20 correct, 20 incorrect (50.0% correct)

Calibration head train accuracy: 0.750
Calibration head confidence range: [0.123, 0.987]
CRC threshold: 0.6543 (alpha=0.05)
```

### 5. Xá»­ lÃ½ edge cases âœ“

**CÃ¡c edge cases Ä‘Æ°á»£c xá»­ lÃ½**:
- âœ… Empty splits â†’ Raise error vá»›i message rÃµ rÃ ng
- âœ… Very small calibration set (< 10 samples) â†’ Warning
- âœ… All labels None â†’ Raise error (khÃ´ng thá»ƒ train)
- âœ… All labels same value â†’ Warning + error náº¿u táº¥t cáº£ correct (khÃ´ng cÃ³ error samples cho CRC)
- âœ… No incorrect samples in calibration â†’ Error (cáº§n Ä‘á»ƒ tÃ­nh CRC threshold)
- âœ… Very few correct/incorrect samples â†’ Warning

### 6. Test script âœ“

**File**: `scripts/test_data_loading.py` (má»›i)

**TÃ­nh nÄƒng**:
- Test data loading tá»« HuggingFace
- Test correctness evaluator vá»›i cÃ¡c test cases
- Test end-to-end pipeline (optional vá»›i `--full` flag)

**CÃ¡ch cháº¡y**:
```bash
# Test cÆ¡ báº£n
python scripts/test_data_loading.py

# Test Ä‘áº§y Ä‘á»§ (bao gá»“m end-to-end)
python scripts/test_data_loading.py --full
```

## ğŸ”§ CÃ¡c cáº£i thiá»‡n chÃ­nh

### 1. Correctness evaluation logic

**TrÆ°á»›c**: Correctness Ä‘Æ°á»£c táº¡o ngáº«u nhiÃªn trong synthetic data

**Sau**: Correctness Ä‘Æ°á»£c tÃ­nh tá»«:
- LLM answer vs reference answers (exact/fuzzy/semantic match)
- Should abstain flag (abstain khi cáº§n = correct)
- Model decision (abstain vs answer)

### 2. Data loading

**TrÆ°á»›c**: Chá»‰ cÃ³ synthetic data

**Sau**: 
- Æ¯u tiÃªn load tá»« HuggingFace (`facebook/AbstentionBench`)
- Tá»± Ä‘á»™ng fallback vá» synthetic náº¿u tháº¥t báº¡i
- Xá»­ lÃ½ nhiá»u format dataset khÃ¡c nhau

### 3. Pipeline validation

**TrÆ°á»›c**: Ãt validation, khÃ³ debug khi cÃ³ lá»—i

**Sau**:
- Validation Ä‘áº§y Ä‘á»§ á»Ÿ má»—i step
- Logging chi tiáº¿t Ä‘á»ƒ debug
- Error messages rÃµ rÃ ng
- Warnings cho cÃ¡c trÆ°á»ng há»£p Ä‘Ã¡ng ngá»

## ğŸ“ CÃ¡ch sá»­ dá»¥ng

### Cháº¡y vá»›i dataset tháº­t

Pipeline sáº½ tá»± Ä‘á»™ng thá»­ load dataset tháº­t. Náº¿u thÃ nh cÃ´ng, báº¡n sáº½ tháº¥y:
```
âœ“ Real dataset detected (has metadata)
Successfully loaded 100 real samples from HuggingFace
```

Náº¿u tháº¥t báº¡i (network issues, dataset khÃ´ng available), sáº½ fallback vá» synthetic:
```
âš  Using synthetic dataset (no real metadata found)
Loaded 100 synthetic samples
```

### Kiá»ƒm tra correctness evaluation

Sau khi cháº¡y pipeline, check logs:
```
[3/9] Evaluating correctness from LLM outputs...
Evaluating correctness for train split (80 samples)...
  train: 80 evaluated (45 correct, 35 incorrect, 0 unknown)
```

### Debug issues

Náº¿u cÃ³ váº¥n Ä‘á», check logs Ä‘á»ƒ xem:
1. **Data loading**: CÃ³ load Ä‘Æ°á»£c dataset tháº­t khÃ´ng?
2. **Correctness evaluation**: CÃ³ evaluate Ä‘Æ°á»£c correctness khÃ´ng?
3. **Label distribution**: Distribution cÃ³ há»£p lÃ½ khÃ´ng?
4. **Calibration**: Calibration head cÃ³ train Ä‘Æ°á»£c khÃ´ng?
5. **CRC threshold**: Threshold cÃ³ há»£p lÃ½ khÃ´ng?

## âš ï¸ LÆ°u Ã½ quan trá»ng

### 1. Dataset availability

Dataset `facebook/AbstentionBench` cÃ³ thá»ƒ:
- Cáº§n `trust_remote_code=True`
- CÃ³ thá»ƒ cáº§n downgrade `datasets` library (<= 3.6.0 theo web search)
- CÃ³ thá»ƒ cÃ³ network issues

**Giáº£i phÃ¡p**: Pipeline tá»± Ä‘á»™ng fallback vá» synthetic náº¿u load tháº¥t báº¡i.

### 2. Correctness evaluation

Correctness Ä‘Æ°á»£c tÃ­nh tá»«:
- Reference answers (náº¿u cÃ³)
- Should abstain flag
- Model decision

**Náº¿u khÃ´ng cÃ³ reference answers**: Correctness sáº½ lÃ  `None` cho nhá»¯ng samples Ä‘Ã³, vÃ  chÃºng sáº½ bá»‹ filter ra khi training.

### 3. Model quality

Vá»›i model nhá» (Qwen2.5-0.5B):
- Selective risk cÃ³ thá»ƒ váº«n cao náº¿u model khÃ´ng Ä‘á»§ máº¡nh
- ÄÃ¢y lÃ  expected behavior, khÃ´ng pháº£i bug
- Äá»ƒ cÃ³ káº¿t quáº£ tá»‘t hÆ¡n, cáº§n model lá»›n hÆ¡n hoáº·c fine-tuned

## ğŸ¯ Káº¿t quáº£ mong Ä‘á»£i

Sau khi tÃ­ch há»£p:
1. âœ… Pipeline load Ä‘Æ°á»£c dataset tháº­t (hoáº·c fallback vá» synthetic)
2. âœ… Correctness Ä‘Æ°á»£c evaluate Ä‘Ãºng tá»« LLM outputs
3. âœ… Validation vÃ  logging Ä‘áº§y Ä‘á»§ Ä‘á»ƒ debug
4. âœ… Edge cases Ä‘Æ°á»£c xá»­ lÃ½ Ä‘Ãºng cÃ¡ch
5. âœ… Káº¿t quáº£ metrics há»£p lÃ½ hÆ¡n (phá»¥ thuá»™c vÃ o model quality)

## ğŸ“Š So sÃ¡nh trÆ°á»›c/sau

| Aspect | TrÆ°á»›c | Sau |
|--------|-------|-----|
| Data source | Synthetic only | Real + fallback |
| Correctness | Random | Evaluated from LLM |
| Validation | Minimal | Comprehensive |
| Logging | Basic | Detailed |
| Edge cases | Not handled | Fully handled |
| Debugging | Difficult | Easy |

## ğŸš€ Next steps

1. **Test vá»›i dataset tháº­t**: Cháº¡y `python scripts/test_data_loading.py --full`
2. **Cháº¡y pilot experiment**: `./scripts/run_pilot.sh`
3. **Kiá»ƒm tra logs**: Xem correctness evaluation vÃ  validation logs
4. **Äiá»u chá»‰nh náº¿u cáº§n**: Model, thresholds, evaluation method

## ğŸ“š Files Ä‘Ã£ thay Ä‘á»•i

1. `src/data/abstention_bench.py` - ThÃªm `_load_real_dataset()`
2. `src/data/correctness_evaluator.py` - File má»›i
3. `src/data/__init__.py` - Export correctness evaluator
4. `src/pipeline/experiment.py` - ThÃªm correctness evaluation step, validation, logging
5. `scripts/test_data_loading.py` - File má»›i Ä‘á»ƒ test

Táº¥t cáº£ thay Ä‘á»•i Ä‘Ã£ Ä‘Æ°á»£c test vÃ  khÃ´ng cÃ³ linter errors! âœ…

